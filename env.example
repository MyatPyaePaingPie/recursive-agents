# Recursive Language Models - Environment Configuration
# Copy this file to .env and fill in your values
# DO NOT commit .env to version control!

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# OpenAI (optional - only if using openai provider)
OPENAI_API_KEY=your-openai-api-key-here

# Anthropic (optional - only if using anthropic provider)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Groq (optional - fast cloud inference, free tier available)
# Get key at: https://console.groq.com/
GROQ_API_KEY=your-groq-api-key-here

# =============================================================================
# LLM Configuration
# =============================================================================

# Provider: ollama (default, local), groq, openai, anthropic
RLM_LLM_PROVIDER=ollama

# Model name (depends on provider)
# Ollama: mistral:7b, llama3:8b, phi3:3.8b
# Groq: llama-3.1-8b-instant, llama-3.1-70b-versatile, mixtral-8x7b-32768
# OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
RLM_LLM_MODEL=mistral:7b

# Generation settings
RLM_LLM_TEMPERATURE=0.7
RLM_LLM_MAX_TOKENS=2000

# Custom base URL (for Ollama or custom endpoints)
# RLM_LLM_BASE_URL=http://localhost:11434/v1

# =============================================================================
# RLM System Configuration
# =============================================================================

# Maximum recursion depth (paper recommends 1)
RLM_MAX_RECURSION_DEPTH=1

# Default chunk size in tokens
RLM_DEFAULT_CHUNK_SIZE=4000

# Chunking strategy: semantic (default), fixed, hierarchical, adaptive
RLM_CHUNKING_STRATEGY=semantic

# Enable LLM response caching
RLM_ENABLE_CACHING=true

# Cache TTL in seconds
RLM_CACHE_TTL=3600

# Logging level: DEBUG, INFO, WARNING, ERROR
RLM_LOG_LEVEL=INFO

# =============================================================================
# Code Execution Security
# =============================================================================

# Execution timeout in seconds (max 60)
RLM_EXEC_TIMEOUT=5

# Memory limit in MB
RLM_EXEC_MEMORY_LIMIT_MB=512

# Enable Docker sandbox for stronger isolation (requires Docker)
RLM_EXEC_ENABLE_DOCKER=false

